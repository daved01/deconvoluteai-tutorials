{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU Score\n",
    "\n",
    "Code for the post [Understanding the BLEU Score for Seq2Seq Model Evaluation](https://deconvoluteai.com/blog/bleu-score).\n",
    "\n",
    "If you have any questions, feel free to reach out to me at [Contact](http://deconvoluteai.com/contact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "The BLEU score calculation works with sentences as the basic unit. For each sentence, we compare it against one or more reference translations.\n",
    "\n",
    "Given these requirements, we'll use a list of lists of strings, where each word is an element of the inner list, and each list represents a sentence. \n",
    "Additionally, we will case-fold all strings and remove punctuation. \n",
    "\n",
    "To construct this list of lists of words, we use this function to convert a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sentences: list[str]) -> list[list[str]]:\n",
    "    \"\"\"Splits a list of strings, for example a list of sentences, into a list of list of strings, and removes punctionation, and case folds.\n",
    "    The input could be a list of at least one sentence as a string.\n",
    "    \n",
    "    Args:\n",
    "        sentences: A list of strings or a list of lists containing strings.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists where each inner list contains words from the corresponding input string.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not type(sentences) == list:\n",
    "        raise ValueError(\"Expecting a list of strings as input.\")\n",
    "    return [[\"\".join(char.lower() for char in word if char.isalnum()) for word in sentence.split()] for sentence in sentences]\n",
    "\n",
    "# Tests\n",
    "assert (preprocess_text([\"Hello, this is some text.\"]) == [['hello', 'this', 'is', 'some', 'text']])\n",
    "assert (preprocess_text([\"Hello, this is some text.\", \"This is a second sentence!\"]) == [['hello', 'this', 'is', 'some', 'text'], ['this', 'is', 'a', 'second', 'sentence']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram Precision\n",
    "\n",
    "Using the preprocessing function, we can ensure that our inputs are always in the correct format. \n",
    "To calculate the n-gram precision, we define a function that takes a `candidate` sentence and a list of `references`, and returns the result as a tuple containing the numerator and denominator. \n",
    "This makes it easier to verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_precision(candidate: list[str], references: list[list[str]], n: int) -> tuple[int, int]:\n",
    "    \"\"\"Calculates the n-gram precision for a candidate given a list of references.\n",
    "    \n",
    "    Args:\n",
    "        candidate: A list of words that make up a sentence.\n",
    "        references: A list of reference sentences, each being a list of words.\n",
    "        n: The parameter for the n-gram.\n",
    "\n",
    "    Returns:\n",
    "        The clipped n-gram count and the total n-gram count.\n",
    "    \"\"\"\n",
    "\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be greater or equal 1.\")\n",
    "\n",
    "    # Count n-grams in candidate\n",
    "    candidate_n_grams = Counter([tuple(candidate[i:i+n]) for i in range(len(candidate)-n+1)])\n",
    "\n",
    "    # Count n-grams in references and take the maximum counts for each n-gram\n",
    "    max_reference_n_grams = Counter()\n",
    "    for reference in references:\n",
    "        reference_n_grams = Counter([tuple(reference[i:i+n]) for i in range(len(reference)-n+1)])\n",
    "        for n_gram in reference_n_grams:\n",
    "            max_reference_n_grams[n_gram] = max(max_reference_n_grams[n_gram], reference_n_grams[n_gram])\n",
    "\n",
    "    # Clip counts\n",
    "    clipped_counts = {ng: min(count, max_reference_n_grams[ng]) for ng, count in candidate_n_grams.items()}\n",
    "    \n",
    "    return sum(clipped_counts.values()), sum(candidate_n_grams.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this function with the examples from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "candidate11 = preprocess_text([\"It is a guide to action which ensures that the military always obeys the commands of the party.\"])\n",
    "candidate12 = preprocess_text([\"It is to insure the troops forever hearing the activity guidebook that party direct.\"])\n",
    "references1 = preprocess_text([\"It is a guide to action that ensures that the military will forever heed Party commands.\", \n",
    "                               \"It is the guiding principle which guarantees the military forces always being under the command of the Party.\",\n",
    "                               \"It is the practical guide for the army always to heed the directions of the party.\"])\n",
    "\n",
    "# Example 2\n",
    "candidate21 = preprocess_text([\"the the the the the the the.\"])\n",
    "references2 = preprocess_text([\"The cat is on the mat.\", \"The cat is on the mat.\"])\n",
    "\n",
    "# Tests\n",
    "assert(n_gram_precision(candidate11[0], references1, 1) == (17,18))\n",
    "assert(n_gram_precision(candidate12[0], references1, 1) == (8,14))\n",
    "assert(n_gram_precision(candidate21[0], references2, 1) == (2,7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brevity Penalty\n",
    "\n",
    "The final part we need is the brevity penalty calculation. \n",
    "This function penalizes shorter candidate translations to avoid rewarding incomplete translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brevity_penalty(candidate: list[str], references: list[list[str]]) -> float:\n",
    "    \"\"\"Calculates the brevity penalty for a candidate sentence against reference translations.\n",
    "    \n",
    "    Args:\n",
    "        candidate: A list of words from the candidate sentence.\n",
    "        references: A list of lists of words from the reference translations.\n",
    "    \n",
    "    Returns:\n",
    "        The brevity penalty score.\n",
    "    \"\"\"\n",
    "    \n",
    "    c = len(candidate)\n",
    "    r = min(len(reference) for reference in references)\n",
    "    \n",
    "    if c > r:\n",
    "        return 1\n",
    "    else:\n",
    "        return math.exp(1 - r / c)\n",
    "\n",
    "# Tests\n",
    "candidate1 = [\"A\", \"test\"]\n",
    "references1 = [[\"A\", \"test\"], [\"Another\", \"test\"] ]\n",
    "references2 = [[\"A\", \"test\", \"hello\"], [\"Another\", \"test\", \"longer\", \"better?\"], [\"And\", \"another\", \"test\", \"longer\", \"worse!\"] ]\n",
    "\n",
    "assert(brevity_penalty(candidate1, references1) == 1.0)\n",
    "assert(brevity_penalty(candidate1, references2) == math.exp(-0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final BLEU Score Calculation\n",
    "\n",
    "Using the three components (n-gram precision, brevity penalty, and geometric mean), we can now calculate the final BLEU score. \n",
    "This function combines all the steps and provides a comprehensive evaluation of the candidate translation against the reference translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score_sentence(candidate_seq: str, references_seqs: list[str], max_n: int=4) -> float:\n",
    "    \"\"\"Calculates the BLEU score with uniform weights for a candidate given a list of references.\n",
    "    \n",
    "    Args:\n",
    "        candidate: A string with the candidate sentence.\n",
    "        references: A list of strings with reference sentences, one string for each sentence.\n",
    "        max_n: The number up to which n-grams should be calculated.\n",
    "    \n",
    "    Returns:\n",
    "        BLEU Score     \n",
    "    \"\"\"\n",
    "\n",
    "    candidate_seq: list[str] = preprocess_text([candidate_seq])[0]\n",
    "    references_seqs: list[list[str]] = preprocess_text(references_seqs)\n",
    "\n",
    "    precisions = []\n",
    "\n",
    "    for n in range(1, max_n+1):\n",
    "        p_n, total = n_gram_precision(candidate_seq, references_seqs, n)\n",
    "        precisions.append(p_n / total if total > 0 else 0)\n",
    "    \n",
    "    if all(p == 0 for p in precisions):\n",
    "        return 0\n",
    "    \n",
    "    geometric_mean = np.exp(np.mean([math.log(p) for p in precisions if p > 0]))\n",
    "    bp = brevity_penalty(candidate_seq, references_seqs)\n",
    "\n",
    "    return bp * geometric_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5045666840058485\n",
      "0.18174699151949172\n",
      "0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "candidate_seq = \"This is a test.\"\n",
    "reference = [\"This is a test.\", \"This is a test.\"]\n",
    "print(bleu_score_sentence(candidate_seq, reference, max_n=4))\n",
    "\n",
    "# Example 2\n",
    "candidate11 = \"It is a guide to action which ensures that the military always obeys the commands of the party.\"\n",
    "candidate12 = \"It is to insure the troops forever hearing the activity guidebook that party direct.\"\n",
    "references1 = [\n",
    "    \"It is a guide to action that ensures that the military will forever heed Party commands.\", \n",
    "    \"It is the guiding principle which guarantees the military forces always being under the command of the Party.\",\n",
    "    \"It is the practical guide for the army always to heed the directions of the party.\"\n",
    "    ]\n",
    "print(bleu_score_sentence(candidate11, references1, 4))\n",
    "print(bleu_score_sentence(candidate12, references1, 4))\n",
    "\n",
    "# Example 3\n",
    "candidate21 = \"the the the the the the the.\"\n",
    "references2 = [\"The cat is on the mat.\", \"The cat is on the mat.\"]\n",
    "\n",
    "print(bleu_score_sentence(candidate21, references2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
